{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def clean_dataset(dataset_path):\n",
    "    for class_folder in [\"ripe\", \"unripe\"]:\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "        for image_file in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "\n",
    "            try:\n",
    "                # Attempt to open the image\n",
    "                img = Image.open(image_path)\n",
    "            except (IOError, SyntaxError):\n",
    "                # Remove corrupt images\n",
    "                # print(f\"Removing corrupt image: {image_path}\")\n",
    "                print(f\"Removing currupt image: {image_path}\")\n",
    "                os.remove(image_path)\n",
    "                continue\n",
    "\n",
    "            # Additional checks and cleaning steps can be added here\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "clean_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def rename_images(dataset_path):\n",
    "    for class_folder in [\"ripe\", \"unripe\"]:\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "        \n",
    "        # Choose a prefix for the renamed images\n",
    "        prefix = class_folder + \"_\"\n",
    "        \n",
    "        # Start the index from 1\n",
    "        index = 1\n",
    "        \n",
    "        for image_file in os.listdir(class_path):\n",
    "            image_path = os.path.join(class_path, image_file)\n",
    "            \n",
    "            # Check if the file is an image\n",
    "            _, file_extension = os.path.splitext(image_file)\n",
    "            if file_extension.lower() not in {\".jpg\", \".jpeg\", \".png\"}:\n",
    "                continue\n",
    "            \n",
    "            # Construct the new filename\n",
    "            new_filename = f\"{prefix}{index:04d}{file_extension.lower()}\"\n",
    "            new_path = os.path.join(class_path, new_filename)\n",
    "            \n",
    "            # Rename the file\n",
    "            os.rename(image_path, new_path)\n",
    "            \n",
    "            # Increment the index\n",
    "            index += 1\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "rename_images(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "def split_dataset(dataset_path, train_size=0.7, validation_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Create directories for training, validation, and test sets\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    validation_dir = os.path.join(dataset_path, 'validation')\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path):\n",
    "            # Get the list of images for the current class\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            \n",
    "            # Split the images into training, validation, and test sets\n",
    "            train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state)\n",
    "            validation_images, test_images = train_test_split(test_images, test_size=(test_size / (validation_size + test_size)), random_state=random_state)\n",
    "\n",
    "            # Move images to their respective directories\n",
    "            for img in train_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(train_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.move(src_path, dest_path)\n",
    "\n",
    "            for img in validation_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(validation_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.move(src_path, dest_path)\n",
    "\n",
    "            for img in test_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(test_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.move(src_path, dest_path)\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "# Split the dataset\n",
    "split_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "\n",
    "def augment_and_split_dataset(dataset_path, train_size=0.7, validation_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Create directories for training, validation, and test sets\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    validation_dir = os.path.join(dataset_path, 'validation')\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Define data augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_size + test_size\n",
    "    )\n",
    "\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path):\n",
    "            # Get the list of images for the current class\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "            # Check if there are images in the class directory\n",
    "            if not images:\n",
    "                print(f\"Skipping class '{class_folder}' due to lack of images.\")\n",
    "                continue\n",
    "\n",
    "            # Split the images into training, validation, and test sets\n",
    "            train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state)\n",
    "            validation_images, test_images = train_test_split(test_images, test_size=(test_size / (validation_size + test_size)), random_state=random_state)\n",
    "\n",
    "            # Set up generators for each split\n",
    "            train_generator = datagen.flow_from_directory(\n",
    "                dataset_path,\n",
    "                target_size=(150, 150),\n",
    "                batch_size=32,\n",
    "                class_mode='binary',  # Use 'binary' for binary classification\n",
    "                subset='training',\n",
    "                classes=[class_folder]\n",
    "            )\n",
    "\n",
    "            validation_generator = datagen.flow_from_directory(\n",
    "                dataset_path,\n",
    "                target_size=(150, 150),\n",
    "                batch_size=32,\n",
    "                class_mode='binary',\n",
    "                subset='validation',\n",
    "                classes=[class_folder]\n",
    "            )\n",
    "\n",
    "            # Move augmented images to their respective directories\n",
    "            for _ in range(len(train_images)):\n",
    "                augmented_image, _ = train_generator.next()\n",
    "                dest_path = os.path.join(train_dir, class_folder, f\"augmented_{len(os.listdir(os.path.join(train_dir, class_folder))) + 1}.jpg\")\n",
    "                shutil.move(augmented_image[0], dest_path)\n",
    "\n",
    "            for _ in range(len(validation_images)):\n",
    "                augmented_image, _ = validation_generator.next()\n",
    "                dest_path = os.path.join(validation_dir, class_folder, f\"augmented_{len(os.listdir(os.path.join(validation_dir, class_folder))) + 1}.jpg\")\n",
    "                shutil.move(augmented_image[0], dest_path)\n",
    "\n",
    "            for img in test_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(test_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.move(src_path, dest_path)\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "# Augment and split the dataset\n",
    "augment_and_split_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "\n",
    "def augment_and_split_dataset(dataset_path, train_size=0.7, validation_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Create directories for training, validation, and test sets\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    validation_dir = os.path.join(dataset_path, 'validation')\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Define data augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_size + test_size\n",
    "    )\n",
    "\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path):\n",
    "            # Get the list of images for the current class\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "            # Check if there are images in the class directory\n",
    "            if not images:\n",
    "                print(f\"Skipping class '{class_folder}' due to lack of images.\")\n",
    "                continue\n",
    "\n",
    "            # Split the images into training, validation, and test sets\n",
    "            train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state)\n",
    "            validation_images, test_images = train_test_split(test_images, test_size=(test_size / (validation_size + test_size)), random_state=random_state)\n",
    "\n",
    "            # Set up generators for each split\n",
    "            train_generator = datagen.flow_from_directory(\n",
    "                dataset_path,\n",
    "                target_size=(150, 150),\n",
    "                batch_size=32,\n",
    "                class_mode='binary',  # Use 'binary' for binary classification\n",
    "                subset='training',\n",
    "                classes=[class_folder]\n",
    "            )\n",
    "\n",
    "            validation_generator = datagen.flow_from_directory(\n",
    "                dataset_path,\n",
    "                target_size=(150, 150),\n",
    "                batch_size=32,\n",
    "                class_mode='binary',\n",
    "                subset='validation',\n",
    "                classes=[class_folder]\n",
    "            )\n",
    "\n",
    "            # Create destination directories for augmented images\n",
    "            train_class_dir = os.path.join(train_dir, class_folder)\n",
    "            os.makedirs(train_class_dir, exist_ok=True)\n",
    "\n",
    "            validation_class_dir = os.path.join(validation_dir, class_folder)\n",
    "            os.makedirs(validation_class_dir, exist_ok=True)\n",
    "\n",
    "            # Move augmented images to their respective directories\n",
    "            for _ in range(len(train_images)):\n",
    "                augmented_image, _ = train_generator.next()\n",
    "                dest_path = os.path.join(train_class_dir, f\"augmented_{len(os.listdir(train_class_dir)) + 1}.jpg\")\n",
    "                shutil.move(augmented_image[0], dest_path)\n",
    "\n",
    "            for _ in range(len(validation_images)):\n",
    "                augmented_image, _ = validation_generator.next()\n",
    "                dest_path = os.path.join(validation_class_dir, f\"augmented_{len(os.listdir(validation_class_dir)) + 1}.jpg\")\n",
    "                shutil.move(augmented_image[0], dest_path)\n",
    "\n",
    "            for img in test_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(test_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.move(src_path, dest_path)\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "# Augment and split the dataset\n",
    "augment_and_split_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "\n",
    "def augment_and_split_dataset(dataset_path, train_size=0.7, validation_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Create directories for training, validation, and test sets\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    validation_dir = os.path.join(dataset_path, 'validation')\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Define data augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_size + test_size\n",
    "    )\n",
    "\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path):\n",
    "            # Get the list of images for the current class\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "            # Check if there are images in the class directory\n",
    "            if not images:\n",
    "                print(f\"Skipping class '{class_folder}' due to lack of images.\")\n",
    "                continue\n",
    "\n",
    "            # Split the images into training, validation, and test sets\n",
    "            train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state)\n",
    "            \n",
    "            # If the unripe class doesn't have enough images for the split, use all available images\n",
    "            if class_folder == 'unripe' and len(train_images) == 0:\n",
    "                train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state, train_size=train_size)\n",
    "            \n",
    "            validation_images, test_images = train_test_split(test_images, test_size=(test_size / (validation_size + test_size)), random_state=random_state)\n",
    "\n",
    "            # Set up generators for each split\n",
    "            train_generator = datagen.flow_from_directory(\n",
    "                dataset_path,\n",
    "                target_size=(150, 150),\n",
    "                batch_size=32,\n",
    "                class_mode='binary',  # Use 'binary' for binary classification\n",
    "                subset='training',\n",
    "                classes=[class_folder]\n",
    "            )\n",
    "\n",
    "            validation_generator = datagen.flow_from_directory(\n",
    "                dataset_path,\n",
    "                target_size=(150, 150),\n",
    "                batch_size=32,\n",
    "                class_mode='binary',\n",
    "                subset='validation',\n",
    "                classes=[class_folder]\n",
    "            )\n",
    "\n",
    "            # Create destination directories for augmented images\n",
    "            train_class_dir = os.path.join(train_dir, class_folder)\n",
    "            os.makedirs(train_class_dir, exist_ok=True)\n",
    "\n",
    "            validation_class_dir = os.path.join(validation_dir, class_folder)\n",
    "            os.makedirs(validation_class_dir, exist_ok=True)\n",
    "\n",
    "            # Move augmented images to their respective directories\n",
    "            for _ in range(len(train_images)):\n",
    "                augmented_image, _ = train_generator.next()\n",
    "                dest_path = os.path.join(train_class_dir, f\"augmented_{len(os.listdir(train_class_dir)) + 1}.jpg\")\n",
    "                shutil.move(augmented_image[0], dest_path)\n",
    "\n",
    "            for _ in range(len(validation_images)):\n",
    "                augmented_image, _ = validation_generator.next()\n",
    "                dest_path = os.path.join(validation_class_dir, f\"augmented_{len(os.listdir(validation_class_dir)) + 1}.jpg\")\n",
    "                shutil.move(augmented_image[0], dest_path)\n",
    "\n",
    "            for img in test_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(test_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.move(src_path, dest_path)\n",
    "\n",
    "# Replace with your actual dataset path\\\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "# Augment and split the dataset\n",
    "augment_and_split_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "def split_unripe_dataset(dataset_path, train_size=0.7, validation_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Create directories for training, validation, and test sets\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    validation_dir = os.path.join(dataset_path, 'validation')\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path):\n",
    "            # Get the list of images for the current class\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "            # Check if there are images in the class directory\n",
    "            if not images:\n",
    "                print(f\"Skipping class '{class_folder}' due to lack of images.\")\n",
    "                continue\n",
    "\n",
    "            # Split the images into training, validation, and test sets\n",
    "            train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state)\n",
    "\n",
    "            # If the unripe class doesn't have enough images for the split, use all available images\n",
    "            if class_folder == 'unripe' and len(train_images) == 0:\n",
    "                train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state, train_size=train_size)\n",
    "\n",
    "            validation_images, test_images = train_test_split(test_images, test_size=(test_size / (validation_size + test_size)), random_state=random_state)\n",
    "\n",
    "            # Move images to their respective directories\n",
    "            for img in train_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(train_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "            for img in validation_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(validation_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "            for img in test_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(test_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "# Split the unripe class without augmentation\n",
    "split_unripe_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "\n",
    "def augment_and_split_dataset(dataset_path, train_size=0.7, validation_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Create directories for training, validation, and test sets\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    validation_dir = os.path.join(dataset_path, 'validation')\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Define data augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_size + test_size\n",
    "    )\n",
    "\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path):\n",
    "            # Get the list of images for the current class\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "            # Check if there are images in the class directory\n",
    "            if not images:\n",
    "                print(f\"Skipping class '{class_folder}' due to lack of images.\")\n",
    "                continue\n",
    "\n",
    "            # Split the images into training, validation, and test sets\n",
    "            train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state)\n",
    "\n",
    "            # If the class doesn't have enough images for the split, use all available images\n",
    "            if len(train_images) == 0:\n",
    "                train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state, train_size=train_size)\n",
    "\n",
    "            validation_images, test_images = train_test_split(test_images, test_size=(test_size / (validation_size + test_size)), random_state=random_state)\n",
    "\n",
    "            # Set up generators for each split\n",
    "            train_generator = datagen.flow_from_directory(\n",
    "                dataset_path,\n",
    "                target_size=(150, 150),\n",
    "                batch_size=32,\n",
    "                class_mode='binary',  # Use 'binary' for binary classification\n",
    "                subset='training',\n",
    "                classes=[class_folder]\n",
    "            )\n",
    "\n",
    "            validation_generator = datagen.flow_from_directory(\n",
    "                dataset_path,\n",
    "                target_size=(150, 150),\n",
    "                batch_size=32,\n",
    "                class_mode='binary',\n",
    "                subset='validation',\n",
    "                classes=[class_folder]\n",
    "            )\n",
    "\n",
    "            # Create destination directories for augmented images\n",
    "            train_class_dir = os.path.join(train_dir, class_folder)\n",
    "            os.makedirs(train_class_dir, exist_ok=True)\n",
    "\n",
    "            validation_class_dir = os.path.join(validation_dir, class_folder)\n",
    "            os.makedirs(validation_class_dir, exist_ok=True)\n",
    "\n",
    "            # Move augmented images to their respective directories\n",
    "            for _ in range(len(train_images)):\n",
    "                augmented_image, _ = train_generator.next()\n",
    "                dest_path = os.path.join(train_class_dir, f\"augmented_{len(os.listdir(train_class_dir)) + 1}.jpg\")\n",
    "                shutil.move(augmented_image[0], dest_path)\n",
    "\n",
    "            for _ in range(len(validation_images)):\n",
    "                augmented_image, _ = validation_generator.next()\n",
    "                dest_path = os.path.join(validation_class_dir, f\"augmented_{len(os.listdir(validation_class_dir)) + 1}.jpg\")\n",
    "                shutil.move(augmented_image[0], dest_path)\n",
    "\n",
    "            for img in test_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(test_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "# Augment and split the dataset\n",
    "augment_and_split_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "\n",
    "def augment_and_split_dataset(dataset_path, train_size=0.7, validation_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Create directories for training, validation, and test sets\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    validation_dir = os.path.join(dataset_path, 'validation')\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Define data augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_size + test_size\n",
    "    )\n",
    "\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path):\n",
    "            # Get the list of images for the current class\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "            # Check if there are images in the class directory\n",
    "            if not images:\n",
    "                print(f\"Skipping class '{class_folder}' due to lack of images.\")\n",
    "                continue\n",
    "\n",
    "            # Split the images into training, validation, and test sets\n",
    "            train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state)\n",
    "\n",
    "            # If the class doesn't have enough images for the split, use all available images\n",
    "            if len(train_images) == 0:\n",
    "                train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state, train_size=train_size)\n",
    "\n",
    "            validation_images, test_images = train_test_split(test_images, test_size=(test_size / (validation_size + test_size)), random_state=random_state)\n",
    "\n",
    "            # Create destination directories for augmented images\n",
    "            train_class_dir = os.path.join(train_dir, class_folder)\n",
    "            os.makedirs(train_class_dir, exist_ok=True)\n",
    "\n",
    "            validation_class_dir = os.path.join(validation_dir, class_folder)\n",
    "            os.makedirs(validation_class_dir, exist_ok=True)\n",
    "\n",
    "            # Move augmented images to their respective directories\n",
    "            for img in train_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(train_class_dir, f\"augmented_{len(os.listdir(train_class_dir)) + 1}.jpg\")\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "            for img in validation_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(validation_class_dir, f\"augmented_{len(os.listdir(validation_class_dir)) + 1}.jpg\")\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "            for img in test_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(test_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "# Augment and split the dataset\n",
    "augment_and_split_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import shutil\n",
    "\n",
    "def augment_and_split_dataset(dataset_path, train_size=0.7, validation_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Create directories for training, validation, and test sets\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    validation_dir = os.path.join(dataset_path, 'validation')\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Define data augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_size + test_size\n",
    "    )\n",
    "\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path) and class_folder in ['ripe', 'unripe']:\n",
    "            # Get the list of images for the current class\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "            # Check if there are images in the class directory\n",
    "            if not images:\n",
    "                print(f\"Skipping class '{class_folder}' due to lack of images.\")\n",
    "                continue\n",
    "\n",
    "            # Split the images into training, validation, and test sets\n",
    "            train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state)\n",
    "\n",
    "            # If the class doesn't have enough images for the split, use all available images\n",
    "            if len(train_images) == 0:\n",
    "                train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state, train_size=train_size)\n",
    "\n",
    "            validation_images, test_images = train_test_split(test_images, test_size=(test_size / (validation_size + test_size)), random_state=random_state)\n",
    "\n",
    "            # Create destination directories for augmented images\n",
    "            train_class_dir = os.path.join(train_dir, class_folder)\n",
    "            os.makedirs(train_class_dir, exist_ok=True)\n",
    "\n",
    "            validation_class_dir = os.path.join(validation_dir, class_folder)\n",
    "            os.makedirs(validation_class_dir, exist_ok=True)\n",
    "\n",
    "            # Move augmented images to their respective directories\n",
    "            for img in train_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(train_class_dir, f\"augmented_{len(os.listdir(train_class_dir)) + 1}.jpg\")\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "            for img in validation_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(validation_class_dir, f\"augmented_{len(os.listdir(validation_class_dir)) + 1}.jpg\")\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "            for img in test_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(test_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "# Augment and split the dataset\n",
    "augment_and_split_dataset(dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "import shutil\n",
    "\n",
    "def augment_and_split_dataset(dataset_path, train_size=0.7, validation_size=0.15, test_size=0.15, random_state=42):\n",
    "    # Create directories for training, validation, and test sets\n",
    "    train_dir = os.path.join(dataset_path, 'train')\n",
    "    validation_dir = os.path.join(dataset_path, 'validation')\n",
    "    test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "    os.makedirs(train_dir, exist_ok=True)\n",
    "    os.makedirs(validation_dir, exist_ok=True)\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "    # Define data augmentation parameters\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=validation_size + test_size\n",
    "    )\n",
    "\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "\n",
    "        if os.path.isdir(class_path) and class_folder in ['ripe', 'unripe']:\n",
    "            # Get the list of images for the current class\n",
    "            images = [img for img in os.listdir(class_path) if img.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "            # Check if there are images in the class directory\n",
    "            if not images:\n",
    "                print(f\"Skipping class '{class_folder}' due to lack of images.\")\n",
    "                continue\n",
    "\n",
    "            # Split the images into training, validation, and test sets\n",
    "            train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state)\n",
    "\n",
    "            # If the class doesn't have enough images for the split, use all available images\n",
    "            if len(train_images) == 0:\n",
    "                train_images, test_images = train_test_split(images, test_size=(validation_size + test_size), random_state=random_state, train_size=train_size)\n",
    "\n",
    "            validation_images, test_images = train_test_split(test_images, test_size=(test_size / (validation_size + test_size)), random_state=random_state)\n",
    "\n",
    "            # Create destination directories for augmented images\n",
    "            train_class_dir = os.path.join(train_dir, class_folder)\n",
    "            os.makedirs(train_class_dir, exist_ok=True)\n",
    "\n",
    "            validation_class_dir = os.path.join(validation_dir, class_folder)\n",
    "            os.makedirs(validation_class_dir, exist_ok=True)\n",
    "\n",
    "            # Move augmented images to their respective directories\n",
    "            for img in train_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(train_class_dir, f\"augmented_{len(os.listdir(train_class_dir)) + 1}.jpg\")\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "            for img in validation_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(validation_class_dir, f\"augmented_{len(os.listdir(validation_class_dir)) + 1}.jpg\")\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "            for img in test_images:\n",
    "                src_path = os.path.join(class_path, img)\n",
    "                dest_path = os.path.join(test_dir, class_folder, img)\n",
    "                os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n",
    "                shutil.copy(src_path, dest_path)\n",
    "\n",
    "# Replace with your actual dataset path\n",
    "dataset_path = \"C:/Users/user/Desktop/project/test\"\n",
    "\n",
    "# Augment and split the dataset\n",
    "augment_and_split_dataset(dataset_path)\n",
    "\n",
    "# Define the directories for training, validation, and test sets\n",
    "train_dir = os.path.join(dataset_path, 'train')\n",
    "validation_dir = os.path.join(dataset_path, 'validation')\n",
    "test_dir = os.path.join(dataset_path, 'test')\n",
    "\n",
    "# Use data generators to load and augment the data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Define the model architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // 32,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // 32\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_acc}\")\n",
    "# Save the model\n",
    "model.save(\"tomato_model.h5\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, model\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(test_generator)\n",
    "predicted_labels = (predictions > 0.5).astype(int)\n",
    "\n",
    "# Get true labels\n",
    "true_labels = test_generator.classes\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['ripe', 'unripe'], yticklabels=['ripe', 'unripe'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_labels, predicted_labels, target_names=['ripe', 'unripe']))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
